{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "xAUBe2fRoeVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Information Gain (IG) is a key metric used to determine the most effective feature for splitting the data at each node of a decision tree (particularly in ID3 and C4.5 algorithms).\n",
        "\n",
        "It measures the reduction in entropy (uncertainty) achieved by splitting the dataset based on a specific feature.\n",
        "\n",
        "A higher Information Gain means the split does a better job of separating the data into purer subsets (nodes where all samples belong to the same class).\n",
        "\n",
        "Usage in Decision Trees: The decision tree algorithm calculates the Information Gain for every available feature and selects the feature that yields the highest Information Gain to make the split at the current node. This greedy approach ensures the tree grows by prioritizing the most informative features first, leading to a more efficient and accurate classification model."
      ],
      "metadata": {
        "id": "r7RY01QSopEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "MvnVDaz-o0l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Gini Impurity and Entropy are measures of node impurity (disorder) used by decision tree algorithms to evaluate the quality of a split.\n",
        "\n",
        "\n",
        "FeatureGini ImpurityEntropyFormula$G(E) = 1 - \\sum_{j=1}^{c}p_{j}^{2}$$H(E) = - \\sum_{j=1}^{c}p_{j}\\log_{2}p_{j}$AlgorithmPrimarily used by the CART (Classification and Regression Trees) algorithm.Primarily used by the ID3 and C4.5 algorithms.Computational CostFaster and simpler to calculate as it avoids logarithmic functions.Slower and more computationally intensive due to the use of logarithmic functions.GoalMeasures the probability of misclassifying a randomly chosen element.Measures the uncertainty or randomness in the data.In PracticeThey produce very similar decision trees, so the choice often comes down to computational efficiency."
      ],
      "metadata": {
        "id": "kaPQGgNso0jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning, also known as early stopping, is a technique used to prevent a decision tree from growing too large and becoming overfitted to the training data.\n",
        "\n",
        "Instead of building the full tree and then trimming it back (post-pruning), pre-pruning imposes constraints that halt the tree's growth during the construction phase.\n",
        "\n",
        "Common criteria for pre-pruning include:\n",
        "\n",
        "Maximum Depth: Limiting the tree to a fixed number of levels.\n",
        "\n",
        "Minimum Samples in a Node: Specifying the minimum number of data points a node must contain before it is allowed to split.\n",
        "\n",
        "Minimum Impurity Decrease: Requiring the split to result in a reduction of impurity (e.g., Gini or Entropy) that exceeds a certain threshold. If the gain is too small, the split is rejected, and the node becomes a leaf."
      ],
      "metadata": {
        "id": "mdV8ycu3o0g7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n"
      ],
      "metadata": {
        "id": "0kCg9Esfo0d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data (good practice, though not strictly required for feature importance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Initialize and train the Decision Tree Classifier with Gini\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the feature importances\n",
        "importances = dt_classifier.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    # Format the importance to 4 decimal places for clean output\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H34JwzVMpnyu",
        "outputId": "71b26667-2842-4d69-b516-6ca663a1604e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm primarily used for classification, but also for regression.\n",
        "\n",
        "Its main objective in a classification task is to find the optimal hyperplane that best separates the data points of different classes in a high-dimensional space.\n",
        "\n",
        "The \"optimal\" hyperplane is the one that maximizes the margin, which is the distance between the hyperplane and the closest data points from each class.\n",
        "\n",
        "The data points that lie closest to the hyperplane (and define the width of the margin) are called the Support Vectors. These vectors are the most critical elements in the training set, as they dictate the position and orientation of the decision boundary."
      ],
      "metadata": {
        "id": "BwYDcE8Ko0Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick is a fundamental technique that allows SVMs to solve non-linear classification problems without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "Problem: If data is not linearly separable in its original low-dimensional space (e.g., a circle of red dots surrounding a cluster of blue dots), a straight-line hyperplane cannot separate them.\n",
        "\n",
        "Solution (The Trick): Instead of manually calculating the coordinates for every data point in a very high-dimensional feature space (which would be computationally expensive), the kernel trick uses a kernel function (like the Radial Basis Function (RBF) or Polynomial) to calculate the dot product between two points as if they were already in that higher dimension.\n",
        "\n",
        "This implicit mapping allows the SVM to find a linear separation (the hyperplane) in the high-dimensional space, which corresponds to a complex, non-linear decision boundary in the original low-dimensional space."
      ],
      "metadata": {
        "id": "ZS-1J1B5o0Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies"
      ],
      "metadata": {
        "id": "a0S8v4arqE-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load and prepare the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data (essential for optimal SVM performance)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Linear Kernel SVM\n",
        "svc_linear = SVC(kernel='linear', random_state=42)\n",
        "svc_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svc_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 3. RBF Kernel SVM\n",
        "svc_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svc_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svc_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 4. Print comparison\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "print(\"\\nComparison:\")\n",
        "if accuracy_rbf > accuracy_linear:\n",
        "    print(\"The RBF kernel performed better for this dataset.\")\n",
        "elif accuracy_linear > accuracy_rbf:\n",
        "    print(\"The Linear kernel performed better for this dataset.\")\n",
        "else:\n",
        "    print(\"Both kernels achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o4a_NNCom4G",
        "outputId": "6e5d7396-8c53-4d76-ffdc-9e0ae3976628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9815\n",
            "Accuracy of SVM with RBF Kernel: 0.9815\n",
            "\n",
            "Comparison:\n",
            "Both kernels achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Naïve Bayes (NB) Classifier: It is a family of probabilistic classifiers based on Bayes' theorem (specifically Maximum A Posteriori (MAP) estimation). It predicts the probability of a given data instance belonging to a particular class. It is often used for text classification, spam filtering, and recommendation systems.\n",
        "\n",
        "Why it is \"Naïve\": It gets its name from the \"Naïve independence assumption\". This assumption states that all features in the dataset are conditionally independent of each other, given the class variable. For example, in an email classification task, it assumes that the presence of the word \"free\" is independent of the presence of the word \"money,\" given that the email is spam. In reality, this assumption is rarely true, yet NB often performs surprisingly well, which is why the model is considered \"Naïve.\""
      ],
      "metadata": {
        "id": "2kt7GUgbqL2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Variant,Feature Type,Distribution Assumption,Common Use Case\n",
        "Gaussian NB,Continuous,Assumes the continuous feature values are sampled from a Normal (Gaussian) distribution for each class.,\"Classification problems where features are continuous, such as the Iris or Breast Cancer datasets.\"\n",
        "Multinomial NB,Discrete/Count,\"Assumes a Multinomial distribution for the features, typically modeling discrete counts (e.g., how many times a word appears).\",\"Text classification, where features are word counts or word frequencies.\"\n",
        "Bernoulli NB,Binary (0 or 1),\"Assumes a Multivariate Bernoulli distribution, where features are independent binary variables, indicating only the presence or absence of a feature (e.g., does a word exist, yes/no).\",\"Text classification, often used when modeling the absence of terms is important.\""
      ],
      "metadata": {
        "id": "qwIjZc7CqRbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "-ZGJ8rt4qfPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# 2. Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print the result\n",
        "print(f\"Gaussian Naïve Bayes Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHdJTCmyqhDZ",
        "outputId": "51377751-7522-4491-aded-c1f37c066a79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Classifier Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}